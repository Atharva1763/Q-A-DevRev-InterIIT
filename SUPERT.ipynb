{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8723440",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39976/721911427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import copy\n",
    "\n",
    "from resources import BASE_DIR, LANGUAGE\n",
    "from ref_free_metrics.similarity_scorer import parse_documents\n",
    "\n",
    "class Supert():\n",
    "    def __init__(self, docs, ref_metric='top15', sim_metric='f1'):\n",
    "        self.bert_model = SentenceTransformer('bert-large-nli-stsb-mean-tokens') \n",
    "        self.sim_metric = sim_metric\n",
    "\n",
    "        # pre-process the documents\n",
    "        self.sent_info_dic, _, self.sents_weights = parse_documents(docs,None,ref_metric)\n",
    "        self.all_token_vecs, self.all_tokens = self.get_all_token_vecs(self.bert_model, self.sent_info_dic)\n",
    "        self.ref_vecs, self.ref_tokens = self.build_pseudo_ref(ref_metric)\n",
    "\n",
    "\n",
    "    def get_all_token_vecs(self, model, sent_info_dict):\n",
    "        all_sents = [sent_info_dict[i]['text'] for i in sent_info_dict]\n",
    "        all_vecs, all_tokens = model.encode(all_sents, token_vecs=True)\n",
    "        assert len(all_vecs) == len(all_tokens)\n",
    "        for i in range(len(all_vecs)):\n",
    "            assert len(all_vecs[i]) == len(all_tokens[i])\n",
    "        return all_vecs, all_tokens\n",
    "\n",
    "\n",
    "    def build_pseudo_ref(self, ref_metric):\n",
    "        ref_dic = {k:self.sent_info_dic[k] for k in self.sent_info_dic if self.sents_weights[k]>=0.1}\n",
    "        # get sents in the pseudo ref\n",
    "        ref_sources = set(ref_dic[k]['doc'] for k in ref_dic)\n",
    "        ref_idxs = []\n",
    "        if len(ref_dic) >= 15: #all(['ref' in rs for rs in ref_sources]):\n",
    "            # group sentences from the same doc into one pseudo ref\n",
    "            for rs in ref_sources:\n",
    "                ref_idxs.append([k for k in ref_dic if ref_dic[k]['doc']==rs])\n",
    "        else:\n",
    "            ref_idxs.append([k for k in ref_dic])\n",
    "        # get vecs and tokens of the pseudo reference\n",
    "        ref_vecs = []\n",
    "        ref_tokens = []\n",
    "        for ref in ref_idxs:\n",
    "            vv, tt = self.kill_stopwords(ref, self.all_token_vecs, self.all_tokens)\n",
    "            ref_vecs.append(vv)\n",
    "            ref_tokens.append(tt)\n",
    "        return ref_vecs, ref_tokens\n",
    "\n",
    "\n",
    "    def __call__(self, summaries):\n",
    "        summ_vecs = []\n",
    "        summ_tokens = []\n",
    "        if isinstance(summaries[0], str):\n",
    "            for summ in summaries:\n",
    "                vv, tt = self.get_token_vecs(self.bert_model, sent_tokenize(summ))\n",
    "                summ_vecs.append(vv)\n",
    "                summ_tokens.append(tt)\n",
    "        elif isinstance(summaries[0], list):\n",
    "            for summ in summaries:\n",
    "                vv, tt = self.kill_stopwords(summ, self.all_token_vecs, self.all_tokens)\n",
    "                summ_vecs.append(vv)\n",
    "                summ_tokens.append(tt)\n",
    "        else:\n",
    "            print('INVALID INPUT SUMMARIES! Should be either a list of strings or a list of integers (indicating the sentence indices)')\n",
    "            exit()\n",
    "        scores = self.get_sbert_score(self.ref_vecs, self.ref_tokens, summ_vecs, summ_tokens, self.sim_metric)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def kill_stopwords(self, sent_idx, all_token_vecs, all_tokens):\n",
    "        for i,si in enumerate(sent_idx):\n",
    "            assert len(all_token_vecs[si]) == len(all_tokens[si])\n",
    "            if i == 0:\n",
    "                full_vec = copy.deepcopy(all_token_vecs[si])\n",
    "                full_token = copy.deepcopy(all_tokens[si])\n",
    "            else:\n",
    "                full_vec = np.row_stack((full_vec, all_token_vecs[si]))\n",
    "                full_token.extend(all_tokens[si])\n",
    "        mystopwords = list(set(stopwords.words(LANGUAGE)))\n",
    "        mystopwords.extend(['[cls]','[sep]'])\n",
    "        wanted_idx = [j for j,tk in enumerate(full_token) if tk.lower() not in mystopwords]\n",
    "        return full_vec[wanted_idx], np.array(full_token)[wanted_idx]\n",
    "\n",
    "\n",
    "    def get_sbert_score(self, ref_token_vecs, ref_tokens, summ_token_vecs, summ_tokens, sim_metric):\n",
    "        recall_list = []\n",
    "        precision_list = []\n",
    "        f1_list = []\n",
    "        empty_summs_ids = []\n",
    "\n",
    "        for i,rvecs in enumerate(ref_token_vecs):\n",
    "            r_recall_list = []\n",
    "            r_precision_list = []\n",
    "            r_f1_list = []\n",
    "            for j,svecs in enumerate(summ_token_vecs):\n",
    "                if svecs is None:\n",
    "                    empty_summs_ids.append(j)\n",
    "                    r_recall_list.append(None)\n",
    "                    r_precision_list.append(None)\n",
    "                    r_f1_list.append(None)\n",
    "                    continue\n",
    "                sim_matrix = cosine_similarity(rvecs,svecs)\n",
    "                recall = np.mean(np.max(sim_matrix, axis=1))\n",
    "                precision = np.mean(np.max(sim_matrix, axis=0))\n",
    "                f1 = 2. * recall * precision / (recall + precision)\n",
    "                r_recall_list.append(recall)\n",
    "                r_precision_list.append(precision)\n",
    "                r_f1_list.append(f1)\n",
    "            recall_list.append(r_recall_list)\n",
    "            precision_list.append(r_precision_list)\n",
    "            f1_list.append(r_f1_list)\n",
    "        empty_summs_ids = list(set(empty_summs_ids))\n",
    "        recall_list = np.array(recall_list)\n",
    "        precision_list = np.array(precision_list)\n",
    "        f1_list = np.array(f1_list)\n",
    "        if 'recall' in sim_metric:\n",
    "            scores = []\n",
    "            for i in range(len(summ_token_vecs)):\n",
    "                if i in empty_summs_ids: scores.append(None)\n",
    "                else: scores.append(np.mean(recall_list[:,i]))\n",
    "            return scores\n",
    "            #return np.mean(np.array(recall_list), axis=0)\n",
    "        elif 'precision' in sim_metric:\n",
    "            scores = []\n",
    "            for i in range(len(summ_token_vecs)):\n",
    "                if i in empty_summs_ids: scores.append(None)\n",
    "                else: scores.append(np.mean(precision_list[:,i]))\n",
    "            return scores\n",
    "            #return np.mean(np.array(precision_list), axis=0)\n",
    "        else:\n",
    "            assert 'f1' in sim_metric\n",
    "            scores = []\n",
    "            for i in range(len(summ_token_vecs)):\n",
    "                if i in empty_summs_ids: scores.append(None)\n",
    "                else: scores.append(np.mean(f1_list[:,i]))\n",
    "            return scores\n",
    "            #return np.mean(np.mean(f1_list),axis=0)\n",
    "\n",
    "    def get_token_vecs(self, model, sents, remove_stopwords=True):\n",
    "        if len(sents) == 0: return None, None\n",
    "        vecs, tokens = model.encode(sents, token_vecs=True)\n",
    "        for i, rtv in enumerate(vecs):\n",
    "            if i==0:\n",
    "                full_vec = rtv\n",
    "                full_token = tokens[i]\n",
    "            else:\n",
    "                full_vec = np.row_stack((full_vec, rtv))\n",
    "                full_token.extend(tokens[i])\n",
    "        if remove_stopwords:\n",
    "            mystopwords = list(set(stopwords.words(LANGUAGE)))\n",
    "            mystopwords.extend(['[cls]','[sep]'])\n",
    "            wanted_idx = [j for j,tk in enumerate(full_token) if tk.lower() not in mystopwords]\n",
    "        else:\n",
    "            wanted_idx = [k for k in range(len(full_token))]\n",
    "        return full_vec[wanted_idx], np.array(full_token)[wanted_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
